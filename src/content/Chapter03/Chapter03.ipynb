{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:159: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:246: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:111: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:123: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:131: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:187: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:192: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:209: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:214: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:297: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:298: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:273: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:283: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:71: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:90: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 105 manually constructing features based on the interaction between them...\n",
      " ['Survived' 'PassengerId' 'Pclass' 'Age' 'SibSp' 'Parch' 'Fare' 'Embarked' 'CabinLetter' 'CabinLetter_0' 'CabinLetter_1' 'CabinLetter_2' 'CabinLetter_3'\n",
      " 'CabinLetter_4' 'CabinLetter_5' 'CabinLetter_6' 'CabinLetter_7' 'CabinLetter_8' 'CabinNumber' 'CabinNumber_scaled' 'TicketPrefixId' 'TicketPrefix_A'\n",
      " 'TicketPrefix_AQ' 'TicketPrefix_AS' 'TicketPrefix_C' 'TicketPrefix_CA' 'TicketPrefix_CASOTON' 'TicketPrefix_FA' 'TicketPrefix_FC' 'TicketPrefix_FCC'\n",
      " 'TicketPrefix_LINE' 'TicketPrefix_LP' 'TicketPrefix_PC' 'TicketPrefix_PP' 'TicketPrefix_PPP' 'TicketPrefix_SC' 'TicketPrefix_SCA' 'TicketPrefix_SCAH'\n",
      " 'TicketPrefix_SCOW' 'TicketPrefix_SCPARIS' 'TicketPrefix_SOC' 'TicketPrefix_SOP' 'TicketPrefix_SOPP' 'TicketPrefix_SOTONO' 'TicketPrefix_SOTONOQ'\n",
      " 'TicketPrefix_SP' 'TicketPrefix_SWPP' 'TicketPrefix_U' 'TicketPrefix_WC' 'TicketPrefix_WEP' 'TicketNumberDigits' 'TicketNumberStart' 'TicketNumber_scaled'\n",
      " 'Names' 'Title_Dr' 'Title_Lady' 'Title_Master' 'Title_Miss' 'Title_Mr' 'Title_Mrs' 'Title_Rev' 'Title_Sir' 'Names_scaled' 'Title_id' 'Title_id_scaled'\n",
      " 'Fare_(0.316, 7.896]' 'Fare_(7.896, 14.454]' 'Fare_(14.454, 31.275]' 'Fare_(31.275, 512.329]' 'Fare_bin_id' 'Fare_scaled' 'Fare_bin_id_scaled' 'Embarked_0'\n",
      " 'Embarked_1' 'Embarked_2' 'SibSp_scaled' 'Parch_scaled' 'SibSp_1' 'SibSp_2' 'SibSp_3' 'SibSp_4' 'SibSp_5' 'SibSp_6' 'SibSp_9' 'Parch_1' 'Parch_2' 'Parch_3'\n",
      " 'Parch_4' 'Parch_5' 'Parch_6' 'Parch_7' 'Parch_10' 'Gender' 'Pclass_1' 'Pclass_2' 'Pclass_3' 'Pclass_scaled' 'Age_scaled' 'isChild' 'Age_(0.169, 21.0]'\n",
      " 'Age_(21.0, 28.035]' 'Age_(28.035, 38.0]' 'Age_(38.0, 80.0]' 'Age_bin_id' 'Age_bin_id_scaled']\n",
      "\n",
      "Using only numeric features for automated feature generation:\n",
      "    Age_scaled  Fare_scaled  Pclass_scaled  Parch_scaled  SibSp_scaled  Names_scaled  CabinNumber_scaled  Age_bin_id_scaled  Fare_bin_id_scaled\n",
      "0     -0.5789      -0.5034         0.8419       -0.4450        0.4813       -0.0755             -0.4207            -1.3561             -1.3233\n",
      "1      0.5982       0.7347        -1.5461       -0.4450        0.4813        2.4586              2.8435            -0.4618             -0.4345\n",
      "2     -0.2847      -0.4903         0.8419       -0.4450       -0.4791       -0.9202             -0.4207            -1.3561              0.4542\n",
      "3      0.3775       0.3831        -1.5461       -0.4450        0.4813        2.4586              4.3027            -0.4618             -0.4345\n",
      "4      0.3775      -0.4879         0.8419       -0.4450       -0.4791       -0.0755             -0.4207            -0.4618              0.4542\n",
      "5     -0.0054      -0.4800         0.8419       -0.4450       -0.4791       -0.9202             -0.4207            -0.4618              0.4542\n",
      "6      1.7753       0.3592        -1.5461       -0.4450       -0.4791       -0.0755              1.3458             0.4324             -0.4345\n",
      "7     -2.0504      -0.2361         0.8419        0.7108        2.4020       -0.0755             -0.4207             1.3267              1.3430\n",
      "8     -0.2111      -0.4283         0.8419        1.8665       -0.4791        2.4586             -0.4207            -1.3561              0.4542\n",
      "9     -1.1675      -0.0622        -0.3521       -0.4450        0.4813        0.7692             -0.4207             1.3267              1.3430\n",
      "\n",
      " 176 new features constructed\n",
      "\n",
      "We are going to drop 44  which are highly correlated features...\n",
      "\n",
      "\n",
      " 237 initial features generated...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:443: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:501: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:502: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass   Age  SibSp  Parch     Fare  Embarked  CabinLetter  CabinLetter_1  CabinLetter_2  CabinLetter_3  CabinLetter_4  CabinLetter_5  CabinLetter_6  CabinLetter_7  CabinLetter_8  CabinNumber  TicketPrefixId  TicketPrefix_A  TicketPrefix_AQ  TicketPrefix_AS  TicketPrefix_C  TicketPrefix_CA  TicketPrefix_CASOTON  TicketPrefix_FA  TicketPrefix_FC  TicketPrefix_FCC  TicketPrefix_LINE  TicketPrefix_LP  TicketPrefix_PC  TicketPrefix_PP  TicketPrefix_PPP  TicketPrefix_SC  TicketPrefix_SCA  TicketPrefix_SCAH  TicketPrefix_SCOW  TicketPrefix_SCPARIS  TicketPrefix_SOC  TicketPrefix_SOP  TicketPrefix_SOPP  TicketPrefix_SOTONO  TicketPrefix_SOTONOQ  TicketPrefix_SP  TicketPrefix_SWPP  TicketPrefix_U  TicketPrefix_WC  TicketPrefix_WEP  TicketNumberDigits  TicketNumberStart  TicketNumber_scaled  Names  Title_Dr  Title_Lady  Title_Master  Title_Miss  Title_Mr  Title_Mrs  Title_Rev  Title_Sir  Title_id  Fare_(0.316, 7.896]  Fare_(7.896, 14.454]  Fare_(14.454, 31.275]  Fare_(31.275, 512.329]  Fare_bin_id  Embarked_1  Embarked_2  SibSp_2  SibSp_3  SibSp_4  SibSp_5  SibSp_6  SibSp_9  Parch_2  Parch_3  Parch_4  Parch_5  Parch_6  Parch_7  Parch_10  Gender  Pclass_1  Pclass_2  Pclass_3  isChild  Age_(0.169, 21.0]  Age_(21.0, 28.035]  Age_(28.035, 38.0]  Age_(38.0, 80.0]  Age_bin_id  Age_scaled*Age_scaled  Age_scaled*Fare_scaled  Age_scaled+Fare_scaled  Age_scaled/Fare_scaled  Age_scaled-Fare_scaled  Age_scaled*Pclass_scaled  Age_scaled+Pclass_scaled  Age_scaled/Pclass_scaled  Age_scaled-Pclass_scaled  Age_scaled*Parch_scaled  Age_scaled+Parch_scaled  Age_scaled/Parch_scaled  Age_scaled-Parch_scaled  Age_scaled*SibSp_scaled  Age_scaled+SibSp_scaled  Age_scaled/SibSp_scaled  Age_scaled-SibSp_scaled  Age_scaled*Names_scaled  Age_scaled+Names_scaled  Age_scaled/Names_scaled  Age_scaled-Names_scaled  Age_scaled*CabinNumber_scaled  Age_scaled+CabinNumber_scaled  Age_scaled/CabinNumber_scaled  Age_scaled-CabinNumber_scaled  Age_scaled*Age_bin_id_scaled  Age_scaled+Age_bin_id_scaled  Age_scaled/Age_bin_id_scaled  Age_scaled-Age_bin_id_scaled  Fare_scaled/Age_scaled  Fare_scaled*Fare_scaled  Fare_scaled*Pclass_scaled  Fare_scaled+Pclass_scaled  Fare_scaled/Pclass_scaled  Fare_scaled-Pclass_scaled  Fare_scaled*Parch_scaled  Fare_scaled+Parch_scaled  Fare_scaled/Parch_scaled  Fare_scaled-Parch_scaled  Fare_scaled*SibSp_scaled  Fare_scaled+SibSp_scaled  Fare_scaled/SibSp_scaled  Fare_scaled-SibSp_scaled  Fare_scaled*Names_scaled  Fare_scaled+Names_scaled  Fare_scaled/Names_scaled  Fare_scaled-Names_scaled  Fare_scaled*CabinNumber_scaled  Fare_scaled+CabinNumber_scaled  Fare_scaled/CabinNumber_scaled  Fare_scaled-CabinNumber_scaled  Fare_scaled*Age_bin_id_scaled  Fare_scaled+Age_bin_id_scaled  Fare_scaled/Age_bin_id_scaled  Fare_scaled-Age_bin_id_scaled  Pclass_scaled/Age_scaled  Pclass_scaled/Fare_scaled  Pclass_scaled*Pclass_scaled  Pclass_scaled*Parch_scaled  Pclass_scaled+Parch_scaled  Pclass_scaled/Parch_scaled  Pclass_scaled-Parch_scaled  Pclass_scaled*SibSp_scaled  Pclass_scaled+SibSp_scaled  Pclass_scaled/SibSp_scaled  Pclass_scaled-SibSp_scaled  Pclass_scaled*Names_scaled  Pclass_scaled+Names_scaled  Pclass_scaled/Names_scaled  Pclass_scaled-Names_scaled  Pclass_scaled*CabinNumber_scaled  Pclass_scaled+CabinNumber_scaled  Pclass_scaled/CabinNumber_scaled  Pclass_scaled-CabinNumber_scaled  Pclass_scaled*Age_bin_id_scaled  Pclass_scaled+Age_bin_id_scaled  Pclass_scaled/Age_bin_id_scaled  Pclass_scaled-Age_bin_id_scaled  Parch_scaled/Age_scaled  Parch_scaled/Fare_scaled  Parch_scaled/Pclass_scaled  Parch_scaled*SibSp_scaled  Parch_scaled+SibSp_scaled  Parch_scaled/SibSp_scaled  Parch_scaled-SibSp_scaled  Parch_scaled*Names_scaled  Parch_scaled+Names_scaled  Parch_scaled/Names_scaled  Parch_scaled-Names_scaled  Parch_scaled*CabinNumber_scaled  Parch_scaled+CabinNumber_scaled  Parch_scaled/CabinNumber_scaled  Parch_scaled-CabinNumber_scaled  Parch_scaled*Age_bin_id_scaled  Parch_scaled+Age_bin_id_scaled  Parch_scaled/Age_bin_id_scaled  Parch_scaled-Age_bin_id_scaled  SibSp_scaled/Age_scaled  SibSp_scaled/Fare_scaled  SibSp_scaled/Pclass_scaled  SibSp_scaled/Parch_scaled  SibSp_scaled*Names_scaled  SibSp_scaled+Names_scaled  SibSp_scaled/Names_scaled  SibSp_scaled-Names_scaled  SibSp_scaled*CabinNumber_scaled  SibSp_scaled+CabinNumber_scaled  SibSp_scaled/CabinNumber_scaled  SibSp_scaled-CabinNumber_scaled  SibSp_scaled*Age_bin_id_scaled  SibSp_scaled+Age_bin_id_scaled  SibSp_scaled/Age_bin_id_scaled  SibSp_scaled-Age_bin_id_scaled  Names_scaled/Age_scaled  Names_scaled/Fare_scaled  Names_scaled/Pclass_scaled  Names_scaled/Parch_scaled  Names_scaled/SibSp_scaled  Names_scaled*Names_scaled  Names_scaled*CabinNumber_scaled  Names_scaled+CabinNumber_scaled  Names_scaled/CabinNumber_scaled  Names_scaled-CabinNumber_scaled  Names_scaled*Age_bin_id_scaled  Names_scaled+Age_bin_id_scaled  Names_scaled/Age_bin_id_scaled  Names_scaled-Age_bin_id_scaled  CabinNumber_scaled/Age_scaled  CabinNumber_scaled/Fare_scaled  CabinNumber_scaled/Pclass_scaled  CabinNumber_scaled/Parch_scaled  CabinNumber_scaled/SibSp_scaled  CabinNumber_scaled/Names_scaled  CabinNumber_scaled*CabinNumber_scaled  CabinNumber_scaled*Age_bin_id_scaled  CabinNumber_scaled+Age_bin_id_scaled  CabinNumber_scaled/Age_bin_id_scaled  CabinNumber_scaled-Age_bin_id_scaled  Age_bin_id_scaled/Age_scaled  Age_bin_id_scaled/Fare_scaled  Age_bin_id_scaled/Pclass_scaled  Age_bin_id_scaled/Parch_scaled  Age_bin_id_scaled/SibSp_scaled  Age_bin_id_scaled/Names_scaled  Age_bin_id_scaled/CabinNumber_scaled  Age_bin_id_scaled*Age_bin_id_scaled\n",
      "0       0.0       3  22.0      2      1   7.2500         0            0              0              0              0              0              0              0              0              0            1               0               1                0                0               0                0                     0                0                0                 0                  0                0                0                0                 0                0                 0                  0                  0                     0                 0                 0                  0                    0                     0                0                  0               0                0                 0                   5                  2              -0.4123      4         0           0             0           0         1          0          0          0         1                    1                     0                      0                       0            1           0           0        1        0        0        0        0        0        0        0        0        0        0        0         0       1         0         0         1        0                  0                   1                   0                 0           1                 0.3352                  0.2914                 -1.0823                  1.1501                 -0.0755                   -0.4874                    0.2630                   -0.6876                   -1.4209                   0.2576                  -1.0239                   1.3010                  -0.1339                  -0.2786                  -0.0976                  -1.2029                  -1.0602                   0.0437                  -0.6544                   7.6679                  -0.5034                         0.2435                        -0.9996                         1.3763                        -0.1583                        0.7851                       -1.9350                        0.4269                        0.7771                  0.8695                   0.2534                    -0.4238                     0.3385                    -0.5979                    -1.3453                    0.2240                   -0.9484                    1.1312                   -0.0584                   -0.2423                   -0.0221                   -1.0459                   -0.9847                    0.0380                   -0.5789                    6.6674                   -0.4279                          0.2118                         -0.9241                          1.1967                         -0.0827                         0.6826                        -1.8595                         0.3712                         0.8527                   -1.4542                    -1.6725                       0.7088                     -0.3747                      0.3969                     -1.8919                      1.2869                      0.4052                      1.3232                      1.7493                      0.3606                     -0.0636                      0.7664                    -11.1511                      0.9174                           -0.3542                            0.4213                           -2.0014                            1.2626                          -1.1417                          -0.5141                          -0.6209                           2.1980                   0.7687                    0.8840                     -0.5286                    -0.2142                     0.0363                    -0.9246                    -0.9263                     0.0336                    -0.5205                     5.8940                    -0.3695                           0.1872                          -0.8657                           1.0579                          -0.0243                          0.6034                         -1.8011                          0.3282                          0.9111                  -0.8313                   -0.9561                      0.5717                    -1.0815                    -0.0363                     0.4058                    -6.3746                     0.5568                          -0.2025                           0.0606                          -1.1441                           0.9019                         -0.6527                         -0.8748                         -0.3549                          1.8374                   0.1304                    0.1500                     -0.0897                     0.1697                    -0.1569                     0.0057                           0.0318                          -0.4962                           0.1795                           0.3452                          0.1024                         -1.4316                          0.0557                          1.2806                         0.7266                          0.8356                           -0.4996                           0.9453                           -0.874                           5.5715                                 0.1769                                0.5704                               -1.7767                                0.3102                                0.9354                        2.3423                         2.6938                          -1.6107                          3.0473                         -2.8176                         17.9609                                3.2237                               1.8389\n",
      "1       1.0       1  38.0      2      1  71.2833         1            1              1              0              0              0              0              0              0              0           86               1               0                0                0               0                0                     0                0                0                 0                  0                0                1                0                 0                0                 0                  0                  0                     0                 0                 0                  0                    0                     0                0                  0               0                0                 0                   5                  1              -0.4180      7         0           0             0           0         0          1          0          0         2                    0                     0                      0                       1            2           1           0        1        0        0        0        0        0        0        0        0        0        0        0         0       0         1         0         0        0                  0                   0                   1                 0           2                 0.3578                  0.4395                  1.3329                  0.8142                 -0.1365                   -0.9249                   -0.9479                   -0.3869                    2.1443                  -0.2662                   0.1532                  -1.3443                   1.0432                   0.2879                   1.0795                   1.2429                   0.1169                   1.4708                   3.0568                   0.2433                  -1.8604                         1.7010                         3.4417                         0.2104                        -2.2452                       -0.2763                        0.1364                       -1.2953                        1.0600                  1.2282                   0.5398                    -1.1359                    -0.8114                    -0.4752                     2.2808                   -0.3269                    0.2897                   -1.6510                    1.1797                    0.3536                    1.2160                    1.5265                    0.2534                    1.8064                    3.1933                    0.2988                   -1.7239                          2.0891                          3.5782                          0.2584                         -2.1088                        -0.3393                         0.2729                        -1.5909                         1.1965                   -2.5846                    -2.1044                       2.3904                      0.6880                     -1.9911                      3.4744                     -1.1011                     -0.7441                     -1.0648                     -3.2124                     -2.0274                     -3.8013                      0.9125                     -0.6288                     -4.0047                           -4.3963                            1.2974                           -0.5437                           -4.3896                           0.7140                          -2.0079                           3.3479                          -1.0843                  -0.7439                   -0.6057                      0.2878                    -0.2142                     0.0363                    -0.9246                    -0.9263                    -1.0941                     2.0136                    -0.1810                    -2.9036                          -1.2653                           2.3985                          -0.1565                          -3.2885                          0.2055                         -0.9068                          0.9636                          0.0168                   0.8046                    0.6551                     -0.3113                    -1.0815                     1.1833                     2.9399                     0.1958                    -1.9773                           1.3685                           3.3247                           0.1693                          -2.3622                         -0.2223                          0.0195                         -1.0422                          0.9431                   4.1100                    3.3464                     -1.5902                    -5.5250                     5.1084                     6.0448                           6.9910                           5.3021                           0.8647                          -0.3848                         -1.1354                          1.9968                         -5.3238                          2.9204                         4.7533                          3.8702                           -1.8391                          -6.3898                            5.908                           1.1565                                 8.0852                               -1.3131                                2.3816                               -6.1572                                3.3053                       -0.7720                        -0.6286                           0.2987                          1.0378                         -0.9595                         -0.1878                               -0.1624                               0.2133\n",
      "2       1.0       3  26.0      1      1   7.9250         0            0              0              0              0              0              0              0              0              0            1               2               0                0                0               0                0                     0                0                0                 0                  0                0                0                0                 0                0                 0                  0                  0                     0                 0                 0                  0                    1                     0                0                  0               0                0                 0                   7                  3               4.4371      3         0           0             0           1         0          0          0          0         3                    0                     1                      0                       0            3           0           0        0        0        0        0        0        0        0        0        0        0        0        0         0       0         0         0         1        0                  0                   1                   0                 0           1                 0.0810                  0.1396                 -0.7750                  0.5805                  0.2057                   -0.2397                    0.5573                   -0.3381                   -1.1266                   0.1267                  -0.7296                   0.6397                   0.1603                   0.1364                  -0.7637                   0.5942                   0.1944                   0.2619                  -1.2049                   0.3093                   0.6356                         0.1197                        -0.7053                         0.6767                         0.1360                        0.3860                       -1.6407                        0.2099                        1.0714                  1.7226                   0.2404                    -0.4128                     0.3516                    -0.5824                    -1.3323                    0.2182                   -0.9353                    1.1019                   -0.0453                    0.2349                   -0.9694                    1.0235                   -0.0113                    0.4512                   -1.4106                    0.5329                    0.4299                          0.2063                         -0.9110                          1.1657                         -0.0697                         0.6649                        -1.8464                         0.3616                         0.8657                   -2.9577                    -1.7170                       0.7088                     -0.3747                      0.3969                     -1.8919                      1.2869                     -0.4034                      0.3628                     -1.7573                      1.3210                     -0.7747                     -0.0783                     -0.9149                      1.7621                           -0.3542                            0.4213                           -2.0014                            1.2626                          -1.1417                          -0.5141                          -0.6209                           2.1980                   1.5633                    0.9075                     -0.5286                     0.2132                    -0.9241                     0.9288                     0.0341                     0.4095                    -1.3652                     0.4836                     0.4752                           0.1872                          -0.8657                           1.0579                          -0.0243                          0.6034                         -1.8011                          0.3282                          0.9111                   1.6831                    0.9770                     -0.5690                     1.0766                     0.4409                    -1.3993                     0.5206                     0.4411                           0.2015                          -0.8997                           1.1389                          -0.0584                          0.6497                         -1.8352                          0.3533                          0.8770                   3.2328                    1.8767                     -1.0930                     2.0679                     1.9208                     0.8468                           0.3871                          -1.3409                           2.1876                          -0.4996                          1.2479                         -2.2763                          0.6786                          0.4359                         1.4778                          0.8579                           -0.4996                           0.9453                            0.878                           0.4571                                 0.1769                                0.5704                               -1.7767                                0.3102                                0.9354                        4.7640                         2.7655                          -1.6107                          3.0473                          2.8305                          1.4736                                3.2237                               1.8389\n",
      "3       1.0       1  35.0      2      1  53.1000         0            1              1              0              0              0              0              0              0              0          124               3               0                0                0               0                0                     0                0                0                 0                  0                0                0                0                 0                0                 0                  0                  0                     0                 0                 0                  0                    0                     0                0                  0               1                0                 0                   6                  1              -0.2665      7         0           0             0           0         0          1          0          0         2                    0                     0                      0                       1            2           0           0        1        0        0        0        0        0        0        0        0        0        0        0         0       0         1         0         0        0                  0                   0                   1                 0           2                 0.1425                  0.1446                  0.7606                  0.9853                 -0.0056                   -0.5836                   -1.1686                   -0.2442                    1.9236                  -0.1680                  -0.0675                  -0.8483                   0.8225                   0.1817                   0.8588                   0.7843                  -0.1038                   0.9281                   2.8361                   0.1535                  -2.0811                         1.6242                         4.6802                         0.0877                        -3.9252                       -0.1743                       -0.0843                       -0.8174                        0.8393                  1.0149                   0.1468                    -0.5923                    -1.1630                    -0.2478                     1.9292                   -0.1705                   -0.0619                   -0.8610                    0.8281                    0.1844                    0.8644                    0.7960                   -0.0982                    0.9420                    2.8417                    0.1558                   -2.0755                          1.6485                          4.6858                          0.0890                         -3.9196                        -0.1769                        -0.0787                        -0.8296                         0.8449                   -4.0957                    -4.0355                       2.3904                      0.6880                     -1.9911                      3.4744                     -1.1011                     -0.7441                     -1.0648                     -3.2124                     -2.0274                     -3.8013                      0.9125                     -0.6288                     -4.0047                           -6.6524                            2.7566                           -0.3593                           -5.8488                           0.7140                          -2.0079                           3.3479                          -1.0843                  -1.1788                   -1.1615                      0.2878                    -0.2142                     0.0363                    -0.9246                    -0.9263                    -1.0941                     2.0136                    -0.1810                    -2.9036                          -1.9147                           3.8577                          -0.1034                          -4.7477                          0.2055                         -0.9068                          0.9636                          0.0168                   1.2750                    1.2562                     -0.3113                    -1.0815                     1.1833                     2.9399                     0.1958                    -1.9773                           2.0708                           4.7840                           0.1119                          -3.8214                         -0.2223                          0.0195                         -1.0422                          0.9431                   6.5130                    6.4173                     -1.5902                    -5.5250                     5.1084                     6.0448                          10.5787                           6.7613                           0.5714                          -1.8441                         -1.1354                          1.9968                         -5.3238                          2.9204                        11.3981                         11.2306                           -2.7829                          -9.6690                            8.940                           1.7500                                18.5133                               -1.9870                                3.8409                               -9.3170                                4.7645                       -1.2234                        -1.2054                           0.2987                          1.0378                         -0.9595                         -0.1878                               -0.1073                               0.2133\n",
      "4       0.0       3  35.0      1      1   8.0500         0            0              0              0              0              0              0              0              0              0            1               3               0                0                0               0                0                     0                0                0                 0                  0                0                0                0                 0                0                 0                  0                  0                     0                 0                 0                  0                    0                     0                0                  0               1                0                 0                   6                  3               0.1423      4         0           0             0           0         1          0          0          0         1                    0                     1                      0                       0            3           0           0        0        0        0        0        0        0        0        0        0        0        0        0         0       1         0         0         1        0                  0                   0                   1                 0           2                 0.1425                 -0.1842                 -0.1104                 -0.7737                  0.8654                    0.3178                    1.2194                    0.4484                   -0.4644                  -0.1680                  -0.0675                  -0.8483                   0.8225                  -0.1809                  -0.1016                  -0.7879                   0.8566                  -0.0285                   0.3020                  -4.9998                   0.4530                        -0.1588                        -0.0432                        -0.8974                         0.7981                       -0.1743                       -0.0843                       -0.8174                        0.8393                 -1.2926                   0.2381                    -0.4108                     0.3540                    -0.5795                    -1.3298                    0.2171                   -0.9329                    1.0965                   -0.0429                    0.2338                   -0.9670                    1.0185                   -0.0088                    0.0368                   -0.5634                    6.4625                   -0.4124                          0.2052                         -0.9086                          1.1599                         -0.0673                         0.2253                        -0.9497                         1.0565                        -0.0261                    2.2303                    -1.7255                       0.7088                     -0.3747                      0.3969                     -1.8919                      1.2869                     -0.4034                      0.3628                     -1.7573                      1.3210                     -0.0636                      0.7664                    -11.1511                      0.9174                           -0.3542                            0.4213                           -2.0014                            1.2626                          -0.3888                           0.3801                          -1.8231                           1.3037                  -1.1788                    0.9120                     -0.5286                     0.2132                    -0.9241                     0.9288                     0.0341                     0.0336                    -0.5205                     5.8940                    -0.3695                           0.1872                          -0.8657                           1.0579                          -0.0243                          0.2055                         -0.9068                          0.9636                          0.0168                  -1.2691                    0.9819                     -0.5690                     1.0766                     0.0362                    -0.5546                     6.3454                    -0.4036                           0.2015                          -0.8997                           1.1389                          -0.0584                          0.2212                         -0.9409                          1.0374                         -0.0173                  -0.2000                    0.1547                     -0.0897                     0.1697                     0.1576                     0.0057                           0.0318                          -0.4962                           0.1795                           0.3452                          0.0349                         -0.5373                          0.1635                          0.3863                        -1.1143                          0.8621                           -0.4996                           0.9453                            0.878                           5.5715                                 0.1769                                0.1943                               -0.8825                                0.9109                                0.0412                       -1.2234                         0.9465                          -0.5485                          1.0378                          0.9639                          6.1167                                1.0978                               0.2133\n",
      "[[   3.       22.        2.        1.        7.25      0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     1.        0.        1.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        5.        2.       -0.4123    4.        0.        0.        0.        0.        1.        0.        0.        0.        1.        1.\n",
      "     0.        0.        0.        1.        0.        0.        1.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        1.        0.        0.        1.        0.        0.        1.        0.        0.        1.        0.3352\n",
      "     0.2914   -1.0823    1.1501   -0.0755   -0.4874    0.263    -0.6876   -1.4209    0.2576   -1.0239    1.301    -0.1339   -0.2786   -0.0976   -1.2029\n",
      "    -1.0602    0.0437   -0.6544    7.6679   -0.5034    0.2435   -0.9996    1.3763   -0.1583    0.7851   -1.935     0.4269    0.7771    0.8695    0.2534\n",
      "    -0.4238    0.3385   -0.5979   -1.3453    0.224    -0.9484    1.1312   -0.0584   -0.2423   -0.0221   -1.0459   -0.9847    0.038    -0.5789    6.6674\n",
      "    -0.4279    0.2118   -0.9241    1.1967   -0.0827    0.6826   -1.8595    0.3712    0.8527   -1.4542   -1.6725    0.7088   -0.3747    0.3969   -1.8919\n",
      "     1.2869    0.4052    1.3232    1.7493    0.3606   -0.0636    0.7664  -11.1511    0.9174   -0.3542    0.4213   -2.0014    1.2626   -1.1417   -0.5141\n",
      "    -0.6209    2.198     0.7687    0.884    -0.5286   -0.2142    0.0363   -0.9246   -0.9263    0.0336   -0.5205    5.894    -0.3695    0.1872   -0.8657\n",
      "     1.0579   -0.0243    0.6034   -1.8011    0.3282    0.9111   -0.8313   -0.9561    0.5717   -1.0815   -0.0363    0.4058   -6.3746    0.5568   -0.2025\n",
      "     0.0606   -1.1441    0.9019   -0.6527   -0.8748   -0.3549    1.8374    0.1304    0.15     -0.0897    0.1697   -0.1569    0.0057    0.0318   -0.4962\n",
      "     0.1795    0.3452    0.1024   -1.4316    0.0557    1.2806    0.7266    0.8356   -0.4996    0.9453   -0.874     5.5715    0.1769    0.5704   -1.7767\n",
      "     0.3102    0.9354    2.3423    2.6938   -1.6107    3.0473   -2.8176   17.9609    3.2237    1.8389]\n",
      " [   1.       38.        2.        1.       71.2833    1.        1.        1.        0.        0.        0.        0.        0.        0.        0.\n",
      "    86.        1.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        5.        1.       -0.418     7.        0.        0.        0.        0.        0.        1.        0.        0.        2.        0.\n",
      "     0.        0.        1.        2.        1.        0.        1.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        1.        0.        0.        0.        0.        0.        1.        0.        2.        0.3578\n",
      "     0.4395    1.3329    0.8142   -0.1365   -0.9249   -0.9479   -0.3869    2.1443   -0.2662    0.1532   -1.3443    1.0432    0.2879    1.0795    1.2429\n",
      "     0.1169    1.4708    3.0568    0.2433   -1.8604    1.701     3.4417    0.2104   -2.2452   -0.2763    0.1364   -1.2953    1.06      1.2282    0.5398\n",
      "    -1.1359   -0.8114   -0.4752    2.2808   -0.3269    0.2897   -1.651     1.1797    0.3536    1.216     1.5265    0.2534    1.8064    3.1933    0.2988\n",
      "    -1.7239    2.0891    3.5782    0.2584   -2.1088   -0.3393    0.2729   -1.5909    1.1965   -2.5846   -2.1044    2.3904    0.688    -1.9911    3.4744\n",
      "    -1.1011   -0.7441   -1.0648   -3.2124   -2.0274   -3.8013    0.9125   -0.6288   -4.0047   -4.3963    1.2974   -0.5437   -4.3896    0.714    -2.0079\n",
      "     3.3479   -1.0843   -0.7439   -0.6057    0.2878   -0.2142    0.0363   -0.9246   -0.9263   -1.0941    2.0136   -0.181    -2.9036   -1.2653    2.3985\n",
      "    -0.1565   -3.2885    0.2055   -0.9068    0.9636    0.0168    0.8046    0.6551   -0.3113   -1.0815    1.1833    2.9399    0.1958   -1.9773    1.3685\n",
      "     3.3247    0.1693   -2.3622   -0.2223    0.0195   -1.0422    0.9431    4.11      3.3464   -1.5902   -5.525     5.1084    6.0448    6.991     5.3021\n",
      "     0.8647   -0.3848   -1.1354    1.9968   -5.3238    2.9204    4.7533    3.8702   -1.8391   -6.3898    5.908     1.1565    8.0852   -1.3131    2.3816\n",
      "    -6.1572    3.3053   -0.772    -0.6286    0.2987    1.0378   -0.9595   -0.1878   -0.1624    0.2133]\n",
      " [   3.       26.        1.        1.        7.925     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     1.        2.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.        0.        0.        0.        0.\n",
      "     0.        7.        3.        4.4371    3.        0.        0.        0.        1.        0.        0.        0.        0.        3.        0.\n",
      "     1.        0.        0.        3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        1.        0.        0.        1.        0.        0.        1.        0.081\n",
      "     0.1396   -0.775     0.5805    0.2057   -0.2397    0.5573   -0.3381   -1.1266    0.1267   -0.7296    0.6397    0.1603    0.1364   -0.7637    0.5942\n",
      "     0.1944    0.2619   -1.2049    0.3093    0.6356    0.1197   -0.7053    0.6767    0.136     0.386    -1.6407    0.2099    1.0714    1.7226    0.2404\n",
      "    -0.4128    0.3516   -0.5824   -1.3323    0.2182   -0.9353    1.1019   -0.0453    0.2349   -0.9694    1.0235   -0.0113    0.4512   -1.4106    0.5329\n",
      "     0.4299    0.2063   -0.911     1.1657   -0.0697    0.6649   -1.8464    0.3616    0.8657   -2.9577   -1.717     0.7088   -0.3747    0.3969   -1.8919\n",
      "     1.2869   -0.4034    0.3628   -1.7573    1.321    -0.7747   -0.0783   -0.9149    1.7621   -0.3542    0.4213   -2.0014    1.2626   -1.1417   -0.5141\n",
      "    -0.6209    2.198     1.5633    0.9075   -0.5286    0.2132   -0.9241    0.9288    0.0341    0.4095   -1.3652    0.4836    0.4752    0.1872   -0.8657\n",
      "     1.0579   -0.0243    0.6034   -1.8011    0.3282    0.9111    1.6831    0.977    -0.569     1.0766    0.4409   -1.3993    0.5206    0.4411    0.2015\n",
      "    -0.8997    1.1389   -0.0584    0.6497   -1.8352    0.3533    0.877     3.2328    1.8767   -1.093     2.0679    1.9208    0.8468    0.3871   -1.3409\n",
      "     2.1876   -0.4996    1.2479   -2.2763    0.6786    0.4359    1.4778    0.8579   -0.4996    0.9453    0.878     0.4571    0.1769    0.5704   -1.7767\n",
      "     0.3102    0.9354    4.764     2.7655   -1.6107    3.0473    2.8305    1.4736    3.2237    1.8389]\n",
      " [   1.       35.        2.        1.       53.1       0.        1.        1.        0.        0.        0.        0.        0.        0.        0.\n",
      "   124.        3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.\n",
      "     0.        6.        1.       -0.2665    7.        0.        0.        0.        0.        0.        1.        0.        0.        2.        0.\n",
      "     0.        0.        1.        2.        0.        0.        1.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        1.        0.        0.        0.        0.        0.        1.        0.        2.        0.1425\n",
      "     0.1446    0.7606    0.9853   -0.0056   -0.5836   -1.1686   -0.2442    1.9236   -0.168    -0.0675   -0.8483    0.8225    0.1817    0.8588    0.7843\n",
      "    -0.1038    0.9281    2.8361    0.1535   -2.0811    1.6242    4.6802    0.0877   -3.9252   -0.1743   -0.0843   -0.8174    0.8393    1.0149    0.1468\n",
      "    -0.5923   -1.163    -0.2478    1.9292   -0.1705   -0.0619   -0.861     0.8281    0.1844    0.8644    0.796    -0.0982    0.942     2.8417    0.1558\n",
      "    -2.0755    1.6485    4.6858    0.089    -3.9196   -0.1769   -0.0787   -0.8296    0.8449   -4.0957   -4.0355    2.3904    0.688    -1.9911    3.4744\n",
      "    -1.1011   -0.7441   -1.0648   -3.2124   -2.0274   -3.8013    0.9125   -0.6288   -4.0047   -6.6524    2.7566   -0.3593   -5.8488    0.714    -2.0079\n",
      "     3.3479   -1.0843   -1.1788   -1.1615    0.2878   -0.2142    0.0363   -0.9246   -0.9263   -1.0941    2.0136   -0.181    -2.9036   -1.9147    3.8577\n",
      "    -0.1034   -4.7477    0.2055   -0.9068    0.9636    0.0168    1.275     1.2562   -0.3113   -1.0815    1.1833    2.9399    0.1958   -1.9773    2.0708\n",
      "     4.784     0.1119   -3.8214   -0.2223    0.0195   -1.0422    0.9431    6.513     6.4173   -1.5902   -5.525     5.1084    6.0448   10.5787    6.7613\n",
      "     0.5714   -1.8441   -1.1354    1.9968   -5.3238    2.9204   11.3981   11.2306   -2.7829   -9.669     8.94      1.75     18.5133   -1.987     3.8409\n",
      "    -9.317     4.7645   -1.2234   -1.2054    0.2987    1.0378   -0.9595   -0.1878   -0.1073    0.2133]\n",
      " [   3.       35.        1.        1.        8.05      0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     1.        3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.\n",
      "     0.        6.        3.        0.1423    4.        0.        0.        0.        0.        1.        0.        0.        0.        1.        0.\n",
      "     1.        0.        0.        3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        1.        0.        0.        1.        0.        0.        0.        1.        0.        2.        0.1425\n",
      "    -0.1842   -0.1104   -0.7737    0.8654    0.3178    1.2194    0.4484   -0.4644   -0.168    -0.0675   -0.8483    0.8225   -0.1809   -0.1016   -0.7879\n",
      "     0.8566   -0.0285    0.302    -4.9998    0.453    -0.1588   -0.0432   -0.8974    0.7981   -0.1743   -0.0843   -0.8174    0.8393   -1.2926    0.2381\n",
      "    -0.4108    0.354    -0.5795   -1.3298    0.2171   -0.9329    1.0965   -0.0429    0.2338   -0.967     1.0185   -0.0088    0.0368   -0.5634    6.4625\n",
      "    -0.4124    0.2052   -0.9086    1.1599   -0.0673    0.2253   -0.9497    1.0565   -0.0261    2.2303   -1.7255    0.7088   -0.3747    0.3969   -1.8919\n",
      "     1.2869   -0.4034    0.3628   -1.7573    1.321    -0.0636    0.7664  -11.1511    0.9174   -0.3542    0.4213   -2.0014    1.2626   -0.3888    0.3801\n",
      "    -1.8231    1.3037   -1.1788    0.912    -0.5286    0.2132   -0.9241    0.9288    0.0341    0.0336   -0.5205    5.894    -0.3695    0.1872   -0.8657\n",
      "     1.0579   -0.0243    0.2055   -0.9068    0.9636    0.0168   -1.2691    0.9819   -0.569     1.0766    0.0362   -0.5546    6.3454   -0.4036    0.2015\n",
      "    -0.8997    1.1389   -0.0584    0.2212   -0.9409    1.0374   -0.0173   -0.2       0.1547   -0.0897    0.1697    0.1576    0.0057    0.0318   -0.4962\n",
      "     0.1795    0.3452    0.0349   -0.5373    0.1635    0.3863   -1.1143    0.8621   -0.4996    0.9453    0.878     5.5715    0.1769    0.1943   -0.8825\n",
      "     0.9109    0.0412   -1.2234    0.9465   -0.5485    1.0378    0.9639    6.1167    1.0978    0.2133]\n",
      " [   3.       29.796     1.        1.        8.4583    2.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     1.        3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.\n",
      "     0.        6.        3.        0.0753    3.        0.        0.        0.        0.        1.        0.        0.        0.        1.        0.\n",
      "     1.        0.        0.        3.        0.        1.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        1.        0.        0.        1.        0.        0.        0.        1.        0.        2.        0.\n",
      "     0.0026   -0.4854    0.0112    0.4747   -0.0045    0.8365   -0.0064   -0.8473    0.0024   -0.4504    0.0121    0.4396    0.0026   -0.4845    0.0112\n",
      "     0.4737    0.0049   -0.9256    0.0058    0.9148    0.0023   -0.426     0.0128    0.4153    0.0025   -0.4672    0.0116    0.4564   89.3161    0.2304\n",
      "    -0.4041    0.3619   -0.5702   -1.322     0.2136   -0.925     1.0787   -0.035     0.23     -0.9591    1.002    -0.0009    0.4417   -1.4002    0.5217\n",
      "     0.4402    0.2019   -0.9007    1.1412   -0.0594    0.2217   -0.9418    1.0395   -0.0182 -156.6489   -1.7539    0.7088   -0.3747    0.3969   -1.8919\n",
      "     1.2869   -0.4034    0.3628   -1.7573    1.321    -0.7747   -0.0783   -0.9149    1.7621   -0.3542    0.4213   -2.0014    1.2626   -0.3888    0.3801\n",
      "    -1.8231    1.3037   82.7976    0.927    -0.5286    0.2132   -0.9241    0.9288    0.0341    0.4095   -1.3652    0.4836    0.4752    0.1872   -0.8657\n",
      "     1.0579   -0.0243    0.2055   -0.9068    0.9636    0.0168   89.14      0.998    -0.569     1.0766    0.4409   -1.3993    0.5206    0.4411    0.2015\n",
      "    -0.8997    1.1389   -0.0584    0.2212   -0.9409    1.0374   -0.0173  171.2159    1.917    -1.093     2.0679    1.9208    0.8468    0.3871   -1.3409\n",
      "     2.1876   -0.4996    0.425    -1.382     1.9926   -0.4584   78.2678    0.8763   -0.4996    0.9453    0.878     0.4571    0.1769    0.1943   -0.8825\n",
      "     0.9109    0.0412   85.926     0.962    -0.5485    1.0378    0.9639    0.5019    1.0978    0.2133]\n",
      " [   1.       54.        1.        1.       51.8625    0.        2.        0.        1.        0.        0.        0.        0.        0.        0.\n",
      "    47.        3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.\n",
      "     0.        5.        1.       -0.4182    4.        0.        0.        0.        0.        1.        0.        0.        0.        1.        0.\n",
      "     0.        0.        1.        2.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        1.        1.        0.        0.        0.        0.        0.        0.        1.        3.        3.1519\n",
      "     0.6377    2.1345    4.9426    1.4162   -2.7449    0.2292   -1.1483    3.3214   -0.79      1.3303   -3.9895    2.2203   -0.8505    1.2963   -3.7057\n",
      "     2.2544   -0.134     1.6998  -23.5142    1.8508    2.3893    3.1212    1.3192    0.4295    0.7677    2.2078    4.1054    1.3429    0.2023    0.129\n",
      "    -0.5554   -1.1869   -0.2323    1.9053   -0.1598   -0.0858   -0.8072    0.8042   -0.1721   -0.1199   -0.7498    0.8383   -0.0271    0.2837   -4.7575\n",
      "     0.4347    0.4834    1.705     0.2669   -0.9866    0.1553    0.7916    0.8306   -0.0732   -0.8709   -4.3043    2.3904    0.688    -1.9911    3.4744\n",
      "    -1.1011    0.7407   -2.0252    3.2272   -1.067     0.1167   -1.6216   20.4779   -1.4706   -2.0807   -0.2003   -1.1488   -2.8919   -0.6686   -1.1137\n",
      "    -3.5753   -1.9785   -0.2507   -1.2389    0.2878    0.2132   -0.9241    0.9288    0.0341    0.0336   -0.5205    5.894    -0.3695   -0.5989    0.9008\n",
      "    -0.3307   -1.7908   -0.1924   -0.0126   -1.029    -0.8774   -0.2699   -1.3338    0.3099    1.0766    0.0362   -0.5546    6.3454   -0.4036   -0.6448\n",
      "     0.8667   -0.356    -1.8249   -0.2072   -0.0466   -1.1079   -0.9115   -0.0425   -0.2102    0.0488    0.1697    0.1576    0.0057   -0.1016    1.2703\n",
      "    -0.0561   -1.4213   -0.0326    0.3569   -0.1746   -0.5079    0.7581    3.7467   -0.8705   -3.0243   -2.8091  -17.825     1.8112    0.582     1.7782\n",
      "     3.1121    0.9134    0.2436    1.2039   -0.2797   -0.9718   -0.9026   -5.7276    0.3213    0.187 ]\n",
      " [   3.        2.        4.        2.       21.075     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     1.        3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.\n",
      "     0.        6.        3.        0.1052    4.        0.        0.        1.        0.        0.        0.        0.        0.        4.        0.\n",
      "     0.        1.        0.        4.        0.        0.        0.        0.        1.        0.        0.        0.        1.        0.        0.\n",
      "     0.        0.        0.        0.        1.        0.        0.        1.        1.        1.        0.        0.        0.        4.        4.204\n",
      "     0.4841   -2.2864    8.6848   -1.8143   -1.7262   -1.2084   -2.4354   -2.8923   -1.4573   -1.3396   -2.8847   -2.7611   -4.925     0.3517   -0.8536\n",
      "    -4.4524    0.1548   -2.1259   27.1568   -1.9749    0.8625   -2.471     4.8742   -1.6297   -2.7202   -0.7237   -1.5455   -3.377     0.1151    0.0557\n",
      "    -0.1988    0.6058   -0.2804   -1.078    -0.1678    0.4747   -0.3322   -0.9469   -0.5671    2.1659   -0.0983   -2.6381    0.0178   -0.3116    3.1269\n",
      "    -0.1606    0.0993   -0.6567    0.5612    0.1846   -0.3132    1.0906   -0.178    -1.5628   -0.4106   -3.5661    0.7088    0.5984    1.5527    1.1845\n",
      "     0.1312    2.0223    3.244     0.3505   -1.5601   -0.0636    0.7664  -11.1511    0.9174   -0.3542    0.4213   -2.0014    1.2626    1.117     2.1686\n",
      "     0.6346   -0.4848   -0.3467   -3.0106    0.8442    1.7073    3.1128    0.2959   -1.6913   -0.0537    0.6353   -9.414     0.7863   -0.299     0.2901\n",
      "    -1.6897    1.1314    0.943     2.0375    0.5357   -0.6159   -1.1715  -10.1744    2.8531    3.3795   -0.1814    2.3265  -31.8147    2.4775   -1.0104\n",
      "     1.9814   -5.7102    2.8227    3.1868    3.7287    1.8106    1.0753    0.0368    0.3198   -0.0897   -0.1062   -0.0314    0.0057    0.0318   -0.4962\n",
      "     0.1795    0.3452   -0.1002    1.2512   -0.0569   -1.4022    0.2052    1.7818   -0.4996   -0.5918   -0.1751    5.5715    0.1769   -0.5581    0.906\n",
      "    -0.3171   -1.7473   -0.6471   -5.6195    1.5758    1.8666    0.5523  -17.5718   -3.1539    1.7601]\n",
      " [   3.       27.        1.        3.       11.1333    0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     1.        3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.\n",
      "     0.        6.        3.        0.1018    7.        0.        0.        0.        0.        0.        1.        0.        0.        2.        0.\n",
      "     1.        0.        0.        3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        1.        0.        0.        1.        0.        0.        1.        0.0446\n",
      "     0.0904   -0.6394    0.4928    0.2172   -0.1777    0.6308   -0.2507   -1.053    -0.394     1.6554   -0.1131   -2.0776    0.1011   -0.6902    0.4406\n",
      "     0.268    -0.519     2.2475   -0.0859   -2.6697    0.0888   -0.6317    0.5018    0.2096    0.2862   -1.5671    0.1557    1.145     2.0292    0.1835\n",
      "    -0.3606    0.4136   -0.5087   -1.2702   -0.7995    1.4382   -0.2295   -2.2948    0.2052   -0.9074    0.894     0.0508   -1.0531    2.0303   -0.1742\n",
      "    -2.8869    0.1802   -0.849     1.0182   -0.0077    0.5808   -1.7844    0.3158    0.9278   -3.9886   -1.9657    0.7088    1.5715    2.7084    0.4511\n",
      "    -1.0246   -0.4034    0.3628   -1.7573    1.321     2.07      3.3005    0.3424   -1.6167   -0.3542    0.4213   -2.0014    1.2626   -1.1417   -0.5141\n",
      "    -0.6209    2.198    -8.8428   -4.3579    2.217    -0.8942    1.3874   -3.896     2.3456    4.5891    4.3251    0.7592   -0.5921   -0.7852    1.4459\n",
      "    -4.4372    2.2872   -2.5311    0.5105   -1.3764    3.2226    2.2697    1.1185   -0.569    -0.2567   -1.1779    1.9795   -0.1949   -2.9377    0.2015\n",
      "    -0.8997    1.1389   -0.0584    0.6497   -1.8352    0.3533    0.877   -11.6479   -5.7403    2.9203    1.3172   -5.1319    6.0448   -1.0342    2.038\n",
      "    -5.8448    2.8793   -3.334     1.1026   -1.8131    3.8147    1.9929    0.9821   -0.4996   -0.2254    0.878    -0.1711    0.1769    0.5704   -1.7767\n",
      "     0.3102    0.9354    6.4244    3.1661   -1.6107   -0.7265    2.8305   -0.5516    3.2237    1.8389]\n",
      " [   2.       14.        2.        1.       30.0708    1.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     1.        3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.\n",
      "     0.        6.        2.       -0.0714    5.        0.        0.        0.        0.        0.        1.        0.        0.        2.        0.\n",
      "     0.        1.        0.        4.        1.        0.        1.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        1.        0.        0.        1.        0.        0.        0.        4.        1.3631\n",
      "     0.0726   -1.2297   18.7849   -1.1054    0.4111   -1.5196    3.3159   -0.8154    0.5195   -1.6125    2.6236   -0.7225   -0.5619   -0.6862   -2.4258\n",
      "    -1.6488   -0.8981   -0.3983   -1.5178   -1.9367    0.4911   -1.5882    2.7755   -0.7469   -1.5489    0.1592   -0.88     -2.4942    0.0532    0.0039\n",
      "     0.0219   -0.4142    0.1765    0.2899    0.0277   -0.5072    0.1397    0.3828   -0.0299    0.4191   -0.1291   -0.5434   -0.0478    0.7071   -0.0808\n",
      "    -0.8314    0.0261   -0.4828    0.1477    0.3585   -0.0825    1.2645   -0.0468   -1.3888    0.3016    5.6651    0.124     0.1567   -0.7971    0.7912\n",
      "     0.0929   -0.1695    0.1292   -0.7316   -0.8334   -0.2708    0.4171   -0.4577   -1.1213    0.1481   -0.7727    0.837     0.0686   -0.4671    0.9746\n",
      "    -0.2654   -1.6788    0.3812    7.16      1.2639   -0.2142    0.0363   -0.9246   -0.9263   -0.3423    0.3242   -0.5785   -1.2142    0.1872   -0.8657\n",
      "     1.0579   -0.0243   -0.5904    0.8817   -0.3354   -1.7717   -0.4122   -7.7438   -1.3669   -1.0815    0.3702    1.2505    0.6257   -0.2879   -0.2025\n",
      "     0.0606   -1.1441    0.9019    0.6385    1.808     0.3628   -0.8454   -0.6588  -12.3764   -2.1847   -1.7286    1.5982    0.5917   -0.3236    0.3486\n",
      "    -1.8286    1.1899    1.0205    2.0959    0.5798   -0.5575    0.3603    6.7682    1.1947    0.9453   -0.874    -0.5469    0.1769   -0.5581    0.906\n",
      "    -0.3171   -1.7473   -1.1363  -21.3461   -3.768    -2.9813    2.7565    1.7248   -3.1539    1.7601]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31  reduced components which describe  .99 % of the variance\n",
      "['Survived' 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:491: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:492: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#threshold=10000,linewidth=160\n",
    "np.set_printoptions(precision=4, threshold=10000, linewidth=160, edgeitems=999, suppress=True)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 160)\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "pd.set_option('precision', 4)\n",
    "\n",
    "\n",
    "#\n",
    "def process_embarked():\n",
    "    global df_titanic_data\n",
    "\n",
    "    #\n",
    "    df_titanic_data.Embarked[df_titanic_data.Embarked.isnull()] = df_titanic_data.Embarked.dropna().mode().values\n",
    "\n",
    "    #\n",
    "    df_titanic_data['Embarked'] = pd.factorize(df_titanic_data['Embarked'])[0]\n",
    "\n",
    "    #\n",
    "    if keep_binary:\n",
    "        df_titanic_data = pd.concat([df_titanic_data, pd.get_dummies(df_titanic_data['Embarked']).rename(\n",
    "            columns=lambda x: 'Embarked_' + str(x))], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#RandomForestClassifierage\n",
    "def set_missing_ages():\n",
    "    global df_titanic_data\n",
    "\n",
    "    age_data = df_titanic_data[\n",
    "        ['Age', 'Embarked', 'Fare', 'Parch', 'SibSp', 'Title_id', 'Pclass', 'Names', 'CabinLetter']]\n",
    "    input_values_RF = age_data.loc[(df_titanic_data.Age.notnull())].values[:, 1::]\n",
    "    target_values_RF = age_data.loc[(df_titanic_data.Age.notnull())].values[:, 0]\n",
    "\n",
    "    #sklearn\n",
    "    regressor = RandomForestRegressor(n_estimators=2000, n_jobs=-1)\n",
    "\n",
    "    #\n",
    "    regressor.fit(input_values_RF, target_values_RF)\n",
    "\n",
    "    #\n",
    "    predicted_ages = regressor.predict(age_data.loc[(df_titanic_data.Age.isnull())].values[:, 1::])\n",
    "\n",
    "    #\n",
    "    #(age_data.loc[(age_data.Age.isnull()), 'Age'] = predicted_ages\n",
    "    \n",
    "    df_titanic_data.loc[(df_titanic_data.Age.isnull()), 'Age'] = predicted_ages\n",
    "    \n",
    "\n",
    "#\n",
    "def process_age():\n",
    "    global df_titanic_data\n",
    "\n",
    "    #set_missing_ages\n",
    "    set_missing_ages()\n",
    "\n",
    "    #\n",
    "    #()\n",
    "    if keep_scaled:\n",
    "        scaler_preprocessing = preprocessing.StandardScaler()\n",
    "        df_titanic_data['Age_scaled'] = scaler_preprocessing.fit_transform(df_titanic_data.Age.reshape(-1, 1))\n",
    "\n",
    "    #\n",
    "    df_titanic_data['isChild'] = np.where(df_titanic_data.Age < 13, 1, 0)\n",
    "\n",
    "    #\n",
    "    df_titanic_data['Age_bin'] = pd.qcut(df_titanic_data['Age'], 4)\n",
    "\n",
    "    if keep_binary:\n",
    "        df_titanic_data = pd.concat(\n",
    "            [df_titanic_data, pd.get_dummies(df_titanic_data['Age_bin']).rename(columns=lambda y: 'Age_' + str(y))],\n",
    "            axis=1)\n",
    "\n",
    "    if keep_bins:\n",
    "        df_titanic_data['Age_bin_id'] = pd.factorize(df_titanic_data['Age_bin'])[0] + 1\n",
    "\n",
    "    if keep_bins and keep_scaled:\n",
    "        scaler_processing = preprocessing.StandardScaler()\n",
    "        df_titanic_data['Age_bin_id_scaled'] = scaler_processing.fit_transform(\n",
    "            df_titanic_data.Age_bin_id.reshape(-1, 1))\n",
    "      \n",
    "    \n",
    "    if not keep_strings:\n",
    "        df_titanic_data.drop('Age_bin', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#/\n",
    "def process_name():\n",
    "    global df_titanic_data\n",
    "\n",
    "    #names\n",
    "    df_titanic_data['Names'] = df_titanic_data['Name'].map(lambda y: len(re.split(' ', y)))\n",
    "\n",
    "    #\n",
    "    df_titanic_data['Title'] = df_titanic_data['Name'].map(lambda y: re.compile(\", (.*?)\\.\").findall(y)[0])\n",
    "\n",
    "    #\n",
    "    df_titanic_data['Title'][df_titanic_data.Title == 'Jonkheer'] = 'Master'\n",
    "    df_titanic_data['Title'][df_titanic_data.Title.isin(['Ms', 'Mlle'])] = 'Miss'\n",
    "    df_titanic_data['Title'][df_titanic_data.Title == 'Mme'] = 'Mrs'\n",
    "    df_titanic_data['Title'][df_titanic_data.Title.isin(['Capt', 'Don', 'Major', 'Col', 'Sir'])] = 'Sir'\n",
    "    df_titanic_data['Title'][df_titanic_data.Title.isin(['Dona', 'Lady', 'the Countess'])] = 'Lady'\n",
    "\n",
    "    #\n",
    "    if keep_binary:\n",
    "        df_titanic_data = pd.concat(\n",
    "            [df_titanic_data, pd.get_dummies(df_titanic_data['Title']).rename(columns=lambda x: 'Title_' + str(x))],\n",
    "            axis=1)\n",
    "\n",
    "    #\n",
    "    if keep_scaled:\n",
    "        scaler_preprocessing = preprocessing.StandardScaler()\n",
    "        df_titanic_data['Names_scaled'] = scaler_preprocessing.fit_transform(df_titanic_data.Names.reshape(-1, 1))\n",
    "\n",
    "    #\n",
    "    if keep_bins:\n",
    "        df_titanic_data['Title_id'] = pd.factorize(df_titanic_data['Title'])[0] + 1\n",
    "\n",
    "    if keep_bins and keep_scaled:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        df_titanic_data['Title_id_scaled'] = scaler.fit_transform(df_titanic_data.Title_id.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "def process_cabin():\n",
    "#\n",
    "    global df_titanic_data\n",
    "\n",
    "    #U0\n",
    "    df_titanic_data['Cabin'][df_titanic_data.Cabin.isnull()] = 'U0'\n",
    "\n",
    "    #\n",
    "    #\n",
    "    df_titanic_data['CabinLetter'] = df_titanic_data['Cabin'].map(lambda l: get_cabin_letter(l))\n",
    "    df_titanic_data['CabinLetter'] = pd.factorize(df_titanic_data['CabinLetter'])[0]\n",
    "\n",
    "    #\n",
    "    if keep_binary:\n",
    "        cletters = pd.get_dummies(df_titanic_data['CabinLetter']).rename(columns=lambda x: 'CabinLetter_' + str(x))\n",
    "        df_titanic_data = pd.concat([df_titanic_data, cletters], axis=1)\n",
    "\n",
    "    #\n",
    "    df_titanic_data['CabinNumber'] = df_titanic_data['Cabin'].map(lambda x: get_cabin_num(x)).astype(int) + 1\n",
    "\n",
    "    #\n",
    "    if keep_scaled:\n",
    "        scaler_processing = preprocessing.StandardScaler()\n",
    "        df_titanic_data['CabinNumber_scaled'] = scaler_processing.fit_transform(df_titanic_data.CabinNumber.reshape(-1, 1))\n",
    "\n",
    "\n",
    "def get_cabin_letter(cabin_value):\n",
    "#\n",
    "    letter_match = re.compile(\"([a-zA-Z]+)\").search(cabin_value)\n",
    "    \n",
    "    if letter_match:\n",
    "        return letter_match.group()\n",
    "    else:\n",
    "        return 'U'\n",
    "\n",
    "\n",
    "def get_cabin_num(cabin_value):\n",
    "#\n",
    "    number_match = re.compile(\"([0-9]+)\").search(cabin_value)\n",
    "\n",
    "    if number_match:\n",
    "        return number_match.group()\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "#\n",
    "def process_fare():\n",
    "    global df_titanic_data\n",
    "\n",
    "    #\n",
    "    df_titanic_data['Fare'][np.isnan(df_titanic_data['Fare'])] = df_titanic_data['Fare'].median()\n",
    "\n",
    "    #0\n",
    "    df_titanic_data['Fare'][np.where(df_titanic_data['Fare'] == 0)[0]] = df_titanic_data['Fare'][\n",
    "                                                                             df_titanic_data['Fare'].nonzero()[\n",
    "                                                                                 0]].min() / 10\n",
    "\n",
    "    #\n",
    "    df_titanic_data['Fare_bin'] = pd.qcut(df_titanic_data['Fare'], 4)\n",
    "\n",
    "    if keep_binary:\n",
    "        df_titanic_data = pd.concat(\n",
    "            [df_titanic_data, pd.get_dummies(df_titanic_data['Fare_bin']).rename(columns=lambda x: 'Fare_' + str(x))],\n",
    "            axis=1)\n",
    "\n",
    "    #\n",
    "    if keep_bins:\n",
    "        df_titanic_data['Fare_bin_id'] = pd.factorize(df_titanic_data['Fare_bin'])[0] + 1\n",
    "\n",
    "    #\n",
    "    if keep_scaled:\n",
    "        scaler_processing = preprocessing.StandardScaler()\n",
    "        df_titanic_data['Fare_scaled'] = scaler_processing.fit_transform(df_titanic_data.Fare.reshape(-1, 1))\n",
    "\n",
    "    if keep_bins and keep_scaled:\n",
    "        scaler_processing = preprocessing.StandardScaler()\n",
    "        df_titanic_data['Fare_bin_id_scaled'] = scaler_processing.fit_transform(\n",
    "            df_titanic_data.Fare_bin_id.reshape(-1, 1))\n",
    "\n",
    "    if not keep_strings:\n",
    "        df_titanic_data.drop('Fare_bin', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#\n",
    "def process_ticket():\n",
    "    global df_titanic_data\n",
    "\n",
    "    df_titanic_data['TicketPrefix'] = df_titanic_data['Ticket'].map(lambda y: get_ticket_prefix(y.upper()))\n",
    "    df_titanic_data['TicketPrefix'] = df_titanic_data['TicketPrefix'].map(lambda y: re.sub('[\\.?\\/?]', '', y))\n",
    "    df_titanic_data['TicketPrefix'] = df_titanic_data['TicketPrefix'].map(lambda y: re.sub('STON', 'SOTON', y))\n",
    "\n",
    "    df_titanic_data['TicketPrefixId'] = pd.factorize(df_titanic_data['TicketPrefix'])[0]\n",
    "\n",
    "    #\n",
    "    if keep_binary:\n",
    "        prefixes = pd.get_dummies(df_titanic_data['TicketPrefix']).rename(columns=lambda y: 'TicketPrefix_' + str(y))\n",
    "        df_titanic_data = pd.concat([df_titanic_data, prefixes], axis=1)\n",
    "\n",
    "    df_titanic_data.drop(['TicketPrefix'], axis=1, inplace=True)\n",
    "\n",
    "    df_titanic_data['TicketNumber'] = df_titanic_data['Ticket'].map(lambda y: get_ticket_num(y))\n",
    "    df_titanic_data['TicketNumberDigits'] = df_titanic_data['TicketNumber'].map(lambda y: len(y)).astype(np.int)\n",
    "    df_titanic_data['TicketNumberStart'] = df_titanic_data['TicketNumber'].map(lambda y: y[0:1]).astype(np.int)\n",
    "\n",
    "    df_titanic_data['TicketNumber'] = df_titanic_data.TicketNumber.astype(np.int)\n",
    "\n",
    "    if keep_scaled:\n",
    "        scaler_processing = preprocessing.StandardScaler()\n",
    "        df_titanic_data['TicketNumber_scaled'] = scaler_processing.fit_transform(\n",
    "            df_titanic_data.TicketNumber.reshape(-1, 1))\n",
    "\n",
    "\n",
    "def get_ticket_prefix(ticket_value):\n",
    "#\n",
    "    match_letter = re.compile(\"([a-zA-Z\\.\\/]+)\").search(ticket_value)\n",
    "    if match_letter:\n",
    "        return match_letter.group()\n",
    "    else:\n",
    "        return 'U'\n",
    "\n",
    "\n",
    "def get_ticket_num(ticket_value):\n",
    "#\n",
    "\n",
    "    match_number = re.compile(\"([\\d]+$)\").search(ticket_value)\n",
    "    if match_number:\n",
    "        return match_number.group()\n",
    "    else:\n",
    "        return '0'\n",
    "\n",
    "\n",
    "    #\n",
    "def process_PClass():\n",
    "    global df_titanic_data\n",
    "\n",
    "    #\n",
    "    df_titanic_data.Pclass[df_titanic_data.Pclass.isnull()] = df_titanic_data.Pclass.dropna().mode().values\n",
    "\n",
    "    #\n",
    "    if keep_binary:\n",
    "        df_titanic_data = pd.concat(\n",
    "            [df_titanic_data, pd.get_dummies(df_titanic_data['Pclass']).rename(columns=lambda y: 'Pclass_' + str(y))],\n",
    "            axis=1)\n",
    "\n",
    "    if keep_scaled:\n",
    "        scaler_preprocessing = preprocessing.StandardScaler()\n",
    "        df_titanic_data['Pclass_scaled'] = scaler_preprocessing.fit_transform(df_titanic_data.Pclass.reshape(-1, 1))\n",
    "\n",
    "\n",
    "    #SibSpParch\n",
    "def process_family():\n",
    "    global df_titanic_data\n",
    "\n",
    "    #\n",
    "    df_titanic_data['SibSp'] = df_titanic_data['SibSp'] + 1\n",
    "    df_titanic_data['Parch'] = df_titanic_data['Parch'] + 1\n",
    "\n",
    "    #\n",
    "    if keep_scaled:\n",
    "        scaler_preprocessing = preprocessing.StandardScaler()\n",
    "        df_titanic_data['SibSp_scaled'] = scaler_preprocessing.fit_transform(df_titanic_data.SibSp.reshape(-1, 1))\n",
    "        df_titanic_data['Parch_scaled'] = scaler_preprocessing.fit_transform(df_titanic_data.Parch.reshape(-1, 1))\n",
    "\n",
    "    #\n",
    "    if keep_binary:\n",
    "        sibsps_var = pd.get_dummies(df_titanic_data['SibSp']).rename(columns=lambda y: 'SibSp_' + str(y))\n",
    "        parchs_var = pd.get_dummies(df_titanic_data['Parch']).rename(columns=lambda y: 'Parch_' + str(y))\n",
    "        df_titanic_data = pd.concat([df_titanic_data, sibsps_var, parchs_var], axis=1)\n",
    "\n",
    "\n",
    "   #\n",
    "def process_sex():\n",
    "    global df_titanic_data\n",
    "    df_titanic_data['Gender'] = np.where(df_titanic_data['Sex'] == 'male', 1, 0)\n",
    "\n",
    "\n",
    "   #\n",
    "def process_drops():\n",
    "    global df_titanic_data\n",
    "    drops = ['Name', 'Names', 'Title', 'Sex', 'SibSp', 'Parch', 'Pclass', 'Embarked', \\\n",
    "             'Cabin', 'CabinLetter', 'CabinNumber', 'Age', 'Fare', 'Ticket', 'TicketNumber']\n",
    "    string_drops = ['Title', 'Name', 'Cabin', 'Ticket', 'Sex', 'Ticket', 'TicketNumber']\n",
    "    if not keep_raw:\n",
    "        df_titanic_data.drop(drops, axis=1, inplace=True)\n",
    "    elif not keep_strings:\n",
    "        df_titanic_data.drop(string_drops, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    #\n",
    "def get_titanic_dataset(binary=False, bins=False, scaled=False, strings=False, raw=True, pca=False, balanced=False):\n",
    "    global keep_binary, keep_bins, keep_scaled, keep_raw, keep_strings, df_titanic_data\n",
    "    keep_binary = binary\n",
    "    keep_bins = bins\n",
    "    keep_scaled = scaled\n",
    "    keep_raw = raw\n",
    "    keep_strings = strings\n",
    "\n",
    "    #Pandas\n",
    "    train_data = pd.read_csv('data/train.csv', header=0)\n",
    "    test_data = pd.read_csv('data/test.csv', header=0)\n",
    "\n",
    "    #\n",
    "    df_titanic_data = pd.concat([train_data, test_data])\n",
    "\n",
    "    #\n",
    "    df_titanic_data.reset_index(inplace=True)\n",
    "\n",
    "    #reset_index\n",
    "    df_titanic_data.drop('index', axis=1, inplace=True)\n",
    "\n",
    "    #1\n",
    "    df_titanic_data = df_titanic_data.reindex_axis(train_data.columns, axis=1)\n",
    "\n",
    "    #titanic\n",
    "    process_cabin()\n",
    "    process_ticket()\n",
    "    process_name()\n",
    "    process_fare()\n",
    "    process_embarked()\n",
    "    process_family()\n",
    "    process_sex()\n",
    "    process_PClass()\n",
    "    process_age()\n",
    "    process_drops()\n",
    "\n",
    "    #servived\n",
    "    columns_list = list(df_titanic_data.columns.values)\n",
    "    columns_list.remove('Survived')\n",
    "    new_col_list = list(['Survived'])\n",
    "    new_col_list.extend(columns_list)\n",
    "    df_titanic_data = df_titanic_data.reindex(columns=new_col_list)\n",
    "    \n",
    "\n",
    "    print(\"Starting with\", df_titanic_data.columns.size,\n",
    "          \"manually constructing features based on the interaction between them...\\n\", df_titanic_data.columns.values)\n",
    "\n",
    "    #\n",
    "    numeric_features = df_titanic_data.loc[:,\n",
    "                       ['Age_scaled', 'Fare_scaled', 'Pclass_scaled', 'Parch_scaled', 'SibSp_scaled',\n",
    "                        'Names_scaled', 'CabinNumber_scaled', 'Age_bin_id_scaled', 'Fare_bin_id_scaled']]\n",
    "    print(\"\\nUsing only numeric features for automated feature generation:\\n\", numeric_features.head(10))\n",
    "\n",
    "    new_fields_count = 0\n",
    "    for i in range(0, numeric_features.columns.size - 1):\n",
    "        for j in range(0, numeric_features.columns.size - 1):\n",
    "            if i <= j:\n",
    "                name = str(numeric_features.columns.values[i]) + \"*\" + str(numeric_features.columns.values[j])\n",
    "                df_titanic_data = pd.concat(\n",
    "                    [df_titanic_data, pd.Series(numeric_features.iloc[:, i] * numeric_features.iloc[:, j], name=name)],\n",
    "                    axis=1)\n",
    "                new_fields_count += 1\n",
    "            if i < j:\n",
    "                name = str(numeric_features.columns.values[i]) + \"+\" + str(numeric_features.columns.values[j])\n",
    "                df_titanic_data = pd.concat(\n",
    "                    [df_titanic_data, pd.Series(numeric_features.iloc[:, i] + numeric_features.iloc[:, j], name=name)],\n",
    "                    axis=1)\n",
    "                new_fields_count += 1\n",
    "            if not i == j:\n",
    "                name = str(numeric_features.columns.values[i]) + \"/\" + str(numeric_features.columns.values[j])\n",
    "                df_titanic_data = pd.concat(\n",
    "                    [df_titanic_data, pd.Series(numeric_features.iloc[:, i] / numeric_features.iloc[:, j], name=name)],\n",
    "                    axis=1)\n",
    "                name = str(numeric_features.columns.values[i]) + \"-\" + str(numeric_features.columns.values[j])\n",
    "                df_titanic_data = pd.concat(\n",
    "                    [df_titanic_data, pd.Series(numeric_features.iloc[:, i] - numeric_features.iloc[:, j], name=name)],\n",
    "                    axis=1)\n",
    "                new_fields_count += 2\n",
    "\n",
    "    print(\"\\n\", new_fields_count, \"new features constructed\")\n",
    "\n",
    "    #Spearman\n",
    "\n",
    "    #\n",
    "    df_titanic_data_cor = df_titanic_data.drop(['Survived', 'PassengerId'], axis=1).corr(method='spearman')\n",
    "\n",
    "    #\n",
    "    mask_ignore = np.ones(df_titanic_data_cor.columns.size) - np.eye(df_titanic_data_cor.columns.size)\n",
    "    df_titanic_data_cor = mask_ignore * df_titanic_data_cor\n",
    "\n",
    "    features_to_drop = []\n",
    "\n",
    "    #\n",
    "    for column in df_titanic_data_cor.columns.values:\n",
    "\n",
    "        #\n",
    "        if np.in1d([column], features_to_drop):\n",
    "            continue\n",
    "\n",
    "        #\n",
    "        corr_vars = df_titanic_data_cor[abs(df_titanic_data_cor[column]) > 0.98].index\n",
    "        features_to_drop = np.union1d(features_to_drop, corr_vars)\n",
    "\n",
    "    print(\"\\nWe are going to drop\", features_to_drop.shape[0], \" which are highly correlated features...\\n\")\n",
    "    df_titanic_data.drop(features_to_drop, axis=1, inplace=True)\n",
    "\n",
    "    #PCA\n",
    "    #()train_data = df_titanic_data[:train_data.shape[0]]\n",
    "    #test_data = df_titanic_data[test_data.shape[0]:]\n",
    "    train_data = df_titanic_data[:train_data.shape[0]]\n",
    "    test_data = df_titanic_data[train_data.shape [0]:]\n",
    "\n",
    "\n",
    "    if pca:\n",
    "        print(\"reducing number of variables...\")\n",
    "        train_data, test_data = reduce(train_data, test_data)\n",
    "    else:\n",
    "        #Survived\n",
    "        test_data.drop('Survived', axis=1, inplace=True)\n",
    "\n",
    "    print(\"\\n\", train_data.columns.size, \"initial features generated...\\n\")  # , input_df.columns.values\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "   #\n",
    "    \n",
    "def reduce(train_data, test_data):\n",
    "    #\n",
    "    df_titanic_data = pd.concat([train_data, test_data])\n",
    "    df_titanic_data.reset_index(inplace=True)\n",
    "    df_titanic_data.drop('index', axis=1, inplace=True)\n",
    "    df_titanic_data = df_titanic_data.reindex_axis(train_data.columns, axis=1)\n",
    "\n",
    "    #survived series\n",
    "    #( survived_series = pd.Series(df['Survived'], name='Survived')\n",
    "    survived_series = pd.Series(df_titanic_data['Survived'], name='Survived')\n",
    "\n",
    "    print(df_titanic_data.head())\n",
    "\n",
    "    #\n",
    "    input_values = df_titanic_data.values[:, 1::]\n",
    "    target_values = df_titanic_data.values[:, 0]\n",
    "\n",
    "    print(input_values[0:10])\n",
    "\n",
    "    #\n",
    "    variance_percentage = .99\n",
    "\n",
    "    #PCA\n",
    "    pca_object = PCA(n_components=variance_percentage)\n",
    "\n",
    "    #\n",
    "    input_values_transformed = pca_object.fit_transform(input_values, target_values)\n",
    "\n",
    "    #PCA\n",
    "    pca_df = pd.DataFrame(input_values_transformed)\n",
    "\n",
    "    print(pca_df.shape[1], \" reduced components which describe \", str(variance_percentage)[1:], \"% of the variance\")\n",
    "\n",
    "    #PCA\n",
    "    df_titanic_data = pd.concat([survived_series, pca_df], axis=1)\n",
    "\n",
    "    #\n",
    "    train_data = df_titanic_data[:train_data.shape[0]]\n",
    "    test_data = df_titanic_data[train_data.shape[0]:]\n",
    "    test_data.reset_index(inplace=True)\n",
    "    test_data.drop('index', axis=1, inplace=True)\n",
    "    test_data.drop('Survived', axis=1, inplace=True)\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "#\n",
    "if __name__ == '__main__':\n",
    "    train,test = get_titanic_dataset(bins=True, scaled=True, binary=True)\n",
    "    initial_drops = ['PassengerId']\n",
    "    train.drop(initial_drops, axis=1, inplace=True)\n",
    "    test.drop(initial_drops, axis=1, inplace=True)\n",
    "\n",
    "    train, test = reduce(train, test)\n",
    "\n",
    "    print(train.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
