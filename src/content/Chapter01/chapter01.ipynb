{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required packages\n",
    "import numpy as np\n",
    "np.random.seed(2016)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import log_loss\n",
    "from keras import __version__ as keras_version\n",
    "\n",
    "\n",
    "# Parameters\n",
    "# ----------\n",
    "# img_path : path\n",
    "#    path of the image to be resized\n",
    "def rezize_image(img_path):\n",
    "   #reading image file\n",
    "   img = cv2.imread(img_path)\n",
    "   #Resize the image to to be 32 by 32\n",
    "   img_resized = cv2.resize(img, (32, 32), cv2.INTER_LINEAR)\n",
    "   return img_resized\n",
    "\n",
    "\n",
    "def load_training_samples():\n",
    "    #Variables to hold the training input and output variables\n",
    "    train_input_variables = []\n",
    "    train_input_variables_id = []\n",
    "    train_label = []\n",
    "    # Scanning all images in each folder of a fish type\n",
    "    print('Start Reading Train Images')\n",
    "    folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "    for fld in folders:\n",
    "       folder_index = folders.index(fld)\n",
    "       print('Load folder {} (Index: {})'.format(fld, folder_index))\n",
    "       imgs_path = os.path.join('..', 'input', 'train', fld, '*.jpg')\n",
    "       files = glob.glob(imgs_path)\n",
    "       for file in files:\n",
    "           file_base = os.path.basename(file)\n",
    "           # Resize the image\n",
    "           resized_img = rezize_image(file)\n",
    "           # Appending the processed image to the input/output variables of the classifier\n",
    "           train_input_variables.append(resized_img)\n",
    "           train_input_variables_id.append(file_base)\n",
    "           train_label.append(folder_index)\n",
    "    return train_input_variables, train_input_variables_id, train_label\n",
    "\n",
    "def load_testing_samples():\n",
    "    # Scanning images from the test folder\n",
    "    imgs_path = os.path.join('..', 'input', 'test_stg1', '*.jpg')\n",
    "    files = sorted(glob.glob(imgs_path))\n",
    "    # Variables to hold the testing samples\n",
    "    testing_samples = []\n",
    "    testing_samples_id = []\n",
    "    #Processing the images and appending them to the array that we have\n",
    "    for file in files:\n",
    "       file_base = os.path.basename(file)\n",
    "       # Image resizing\n",
    "       resized_img = rezize_image(file)\n",
    "       testing_samples.append(resized_img)\n",
    "       testing_samples_id.append(file_base)\n",
    "    return testing_samples, testing_samples_id\n",
    "\n",
    "\n",
    "def load_normalize_training_samples():\n",
    "    # Calling the load function in order to load and resize the training samples\n",
    "    training_samples, training_label, training_samples_id = load_training_samples()\n",
    "    # Converting the loaded and resized data into Numpy format\n",
    "    training_samples = np.array(training_samples, dtype=np.uint8)\n",
    "    training_label = np.array(training_label, dtype=np.uint8)\n",
    "    # Reshaping the training samples\n",
    "    training_samples = training_samples.transpose((0, 3, 1, 2))\n",
    "    # Converting the training samples and training labels into float format\n",
    "    training_samples = training_samples.astype('float32')\n",
    "    training_samples = training_samples / 255\n",
    "    training_label = np_utils.to_categorical(training_label, 8)\n",
    "    return training_samples, training_label, training_samples_id\n",
    "\n",
    "def load_normalize_testing_samples():\n",
    "    # Calling the load function in order to load and resize the testing samples\n",
    "    testing_samples, testing_samples_id = load_testing_samples()\n",
    "    # Converting the loaded and resized data into Numpy format\n",
    "    testing_samples = np.array(testing_samples, dtype=np.uint8)\n",
    "    # Reshaping the testing samples\n",
    "    testing_samples = testing_samples.transpose((0, 3, 1, 2))\n",
    "    # Converting the testing samples into float format\n",
    "    testing_samples = testing_samples.astype('float32')\n",
    "    testing_samples = testing_samples / 255\n",
    "    return testing_samples, testing_samples_id\n",
    "\n",
    "def create_cnn_model_arch():\n",
    "    pool_size = 2 # we will use 2x2 pooling throughout\n",
    "    conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...\n",
    "    conv_depth_2 = 64 # ...switching to 64 after the first pooling layer\n",
    "    kernel_size = 3 # we will use 3x3 kernels throughout\n",
    "    drop_prob = 0.5 # dropout in the FC layer with probability 0.5\n",
    "    hidden_size = 32 # the FC layer will have 512 neurons\n",
    "    num_classes = 8 # there are 8 fish types\n",
    "    # Conv [32] -> Conv [32] -> Pool\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(ZeroPadding2D((1, 1), input_shape=(3, 32, 32), dim_ordering='th'))\n",
    "    cnn_model.add(Convolution2D(conv_depth_1, kernel_size, kernel_size, activation='relu',\n",
    "      dim_ordering='th'))\n",
    "    cnn_model.add(ZeroPadding2D((1, 1), dim_ordering='th'))\n",
    "    cnn_model.add(Convolution2D(conv_depth_1, kernel_size, kernel_size, activation='relu',\n",
    "      dim_ordering='th'))\n",
    "    cnn_model.add(MaxPooling2D(pool_size=(pool_size, pool_size), strides=(2, 2),\n",
    "      dim_ordering='th'))\n",
    "    # Conv [64] -> Conv [64] -> Pool\n",
    "    cnn_model.add(ZeroPadding2D((1, 1), dim_ordering='th'))\n",
    "    cnn_model.add(Convolution2D(conv_depth_2, kernel_size, kernel_size, activation='relu',\n",
    "      dim_ordering='th'))\n",
    "    cnn_model.add(ZeroPadding2D((1, 1), dim_ordering='th'))\n",
    "    cnn_model.add(Convolution2D(conv_depth_2, kernel_size, kernel_size, activation='relu',\n",
    "      dim_ordering='th'))\n",
    "    cnn_model.add(MaxPooling2D(pool_size=(pool_size, pool_size), strides=(2, 2),\n",
    "     dim_ordering='th'))\n",
    "    # Now flatten to 1D, apply FC then ReLU (with dropout) and finally softmax(output layer)\n",
    "    cnn_model.add(Flatten())\n",
    "    cnn_model.add(Dense(hidden_size, activation='relu'))\n",
    "    cnn_model.add(Dropout(drop_prob))\n",
    "    cnn_model.add(Dense(hidden_size, activation='relu'))\n",
    "    cnn_model.add(Dropout(drop_prob))\n",
    "    cnn_model.add(Dense(num_classes, activation='softmax'))\n",
    "    # initiating the stochastic gradient descent optimiser\n",
    "    stochastic_gradient_descent = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)    cnn_model.compile(optimizer=stochastic_gradient_descent,  # using the stochastic gradient descent optimiser\n",
    "                  loss='categorical_crossentropy')  # using the cross-entropy loss function\n",
    "    return cnn_model\n",
    "\n",
    "def create_model_with_kfold_cross_validation(nfolds=10):\n",
    "    batch_size = 16 # in each iteration, we consider 32 training examples at once\n",
    "    num_epochs = 30 # we iterate 200 times over the entire training set\n",
    "    random_state = 51 # control the randomness for reproducibility of the results on the same platform\n",
    "    # Loading and normalizing the training samples prior to feeding it to the created CNN model\n",
    "    training_samples, training_samples_target, training_samples_id =\n",
    "      load_normalize_training_samples()\n",
    "    yfull_train = dict()\n",
    "    # Providing Training/Testing indices to split data in the training samples\n",
    "    # which is splitting data into 10 consecutive folds with shuffling\n",
    "    kf = KFold(len(train_id), n_folds=nfolds, shuffle=True, random_state=random_state)\n",
    "    fold_number = 0 # Initial value for fold number\n",
    "    sum_score = 0 # overall score (will be incremented at each iteration)\n",
    "    trained_models = [] # storing the modeling of each iteration over the folds\n",
    "    # Getting the training/testing samples based on the generated training/testing indices by\n",
    "      Kfold\n",
    "    for train_index, test_index in kf:\n",
    "       cnn_model = create_cnn_model_arch()\n",
    "       training_samples_X = training_samples[train_index] # Getting the training input variables\n",
    "       training_samples_Y = training_samples_target[train_index] # Getting the training output/label variable\n",
    "       validation_samples_X = training_samples[test_index] # Getting the validation input variables\n",
    "       validation_samples_Y = training_samples_target[test_index] # Getting the validation output/label variable\n",
    "       fold_number += 1\n",
    "       print('Fold number {} from {}'.format(fold_number, nfolds))\n",
    "       callbacks = [\n",
    "           EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "       ]\n",
    "       # Fitting the CNN model giving the defined settings\n",
    "       cnn_model.fit(training_samples_X, training_samples_Y, batch_size=batch_size,\n",
    "         nb_epoch=num_epochs,\n",
    "             shuffle=True, verbose=2, validation_data=(validation_samples_X,\n",
    "               validation_samples_Y),\n",
    "             callbacks=callbacks)\n",
    "       # measuring the generalization ability of the trained model based on the validation set\n",
    "       predictions_of_validation_samples =\n",
    "         cnn_model.predict(validation_samples_X.astype('float32'),\n",
    "         batch_size=batch_size, verbose=2)\n",
    "       current_model_score = log_loss(Y_valid, predictions_of_validation_samples)\n",
    "       print('Current model score log_loss: ', current_model_score)\n",
    "       sum_score += current_model_score*len(test_index)\n",
    "       # Store valid predictions\n",
    "       for i in range(len(test_index)):\n",
    "           yfull_train[test_index[i]] = predictions_of_validation_samples[i]\n",
    "       # Store the trained model\n",
    "       trained_models.append(cnn_model)\n",
    "    # incrementing the sum_score value by the current model calculated score\n",
    "    overall_score = sum_score/len(training_samples)\n",
    "    print(\"Log_loss train independent avg: \", overall_score)\n",
    "    #Reporting the model loss at this stage\n",
    "    overall_settings_output_string = 'loss_' + str(overall_score) + '_folds_' + str(nfolds) + '_ep_' + str(num_epochs)\n",
    "    return overall_settings_output_string, trained_models\n",
    "\n",
    "def test_generality_crossValidation_over_test_set( overall_settings_output_string, cnn_models):\n",
    "    batch_size = 16 # in each iteration, we consider 32 training examples at once\n",
    "    fold_number = 0 # fold iterator\n",
    "    number_of_folds = len(cnn_models) # Creating number of folds based on the value used in the training step\n",
    "    yfull_test = [] # variable to hold overall predictions for the test set\n",
    "    #executing the actual cross validation test process over the test set\n",
    "    for j in range(number_of_folds):\n",
    "       model = cnn_models[j]\n",
    "       fold_number += 1\n",
    "       print('Fold number {} out of {}'.format(fold_number, number_of_folds))\n",
    "       #Loading and normalizing testing samples\n",
    "       testing_samples, testing_samples_id = load_normalize_testing_samples()\n",
    "       #Calling the current model over the current test fold\n",
    "       test_prediction = model.predict(testing_samples, batch_size=batch_size, verbose=2)\n",
    "       yfull_test.append(test_prediction)\n",
    "    test_result = merge_several_folds_mean(yfull_test, number_of_folds)\n",
    "    overall_settings_output_string = 'loss_' + overall_settings_output_string \\ + '_folds_' +\n",
    "      str(number_of_folds)\n",
    "    format_results_for_types(test_result, testing_samples_id, overall_settings_output_string)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  info_string, models = create_model_with_kfold_cross_validation()\n",
    "  test_generality_crossValidation_over_test_set(info_string, models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
